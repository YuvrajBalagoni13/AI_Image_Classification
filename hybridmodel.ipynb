{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fff5ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n",
      "0.21.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as Timer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset, DataLoader, ConcatDataset, Dataset\n",
    "from torchinfo import summary\n",
    "import wandb\n",
    "import onnx\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aac692bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f2c05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"dataset/\")\n",
    "train_dataset_path = data_path/\"train\"\n",
    "test_dataset_path = data_path/\"test\"\n",
    "val_dataset_path = data_path/\"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8ad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 84000\n",
       "     Root location: dataset\\train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 24000\n",
       "     Root location: dataset\\val\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 12000\n",
       "     Root location: dataset\\test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root= train_dataset_path,\n",
    "                                  transform= transform,\n",
    "                                  target_transform= None)\n",
    "val_data = datasets.ImageFolder(root= val_dataset_path,\n",
    "                                transform= transform,\n",
    "                                target_transform= None)\n",
    "test_data = datasets.ImageFolder(root= test_dataset_path,\n",
    "                                 transform= transform)\n",
    "\n",
    "train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7133c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "387bfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.Layer = torch.nn.Sequential(\n",
    "            nn.Conv2d(input_shape, hidden_units, kernel_size= (3,3)),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size= (3,3)),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size= (2,2)),\n",
    "            nn.Conv2d(hidden_units, output_shape, kernel_size=(5,5)),\n",
    "            nn.BatchNorm2d(output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62c21c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 256)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CNNBlock output is (64, 10, 10)\n",
    "H,W,C,P = 10, 10, 64, 2\n",
    "N = int(H * W / (P ** 2))\n",
    "D = P * P * C\n",
    "N, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b98ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 patch_size: int,\n",
    "                 embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.patcher = nn.Conv2d(in_channels= in_channels,\n",
    "                                 out_channels= embedding_dim,\n",
    "                                 stride= patch_size,\n",
    "                                 kernel_size= patch_size,\n",
    "                                 padding= 0)\n",
    "        self.flatten = nn.Flatten(start_dim= 2,\n",
    "                                  end_dim= 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        image_res = x.shape[-1]\n",
    "        assert (image_res % self.patch_size == 0), \"patch size should be divisible with image resolution\"\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "        return x_flattened.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30bf9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim : int,\n",
    "                 num_heads : int,\n",
    "                 att_dropout : float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "        self.MultiHeadAttention = nn.MultiheadAttention(embed_dim= embedding_dim,\n",
    "                                                        num_heads= num_heads,\n",
    "                                                        dropout= att_dropout,\n",
    "                                                        batch_first= True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.LayerNorm(x)\n",
    "        attn_output, _ = self.MultiHeadAttention(query= x,\n",
    "                                                 key= x,\n",
    "                                                 value= x,\n",
    "                                                 need_weights = False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74189d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPreceptronBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 mlp_size: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(in_features= embedding_dim,\n",
    "                      out_features= mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features= mlp_size,\n",
    "                      out_features= embedding_dim),\n",
    "            nn.Dropout(p= dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.LayerNorm(x)\n",
    "        x = self.MLP(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1217282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_size: int,\n",
    "                 attn_dropout: float,\n",
    "                 mlp_dropout: float):\n",
    "        super().__init__()\n",
    "        self.MSA_Block = MultiHeadSelfAttentionBlock(embedding_dim= embedding_dim,\n",
    "                                               num_heads= num_heads,\n",
    "                                               att_dropout= attn_dropout)\n",
    "        self.MLP_Block = MultiLayerPreceptronBlock(embedding_dim= embedding_dim,\n",
    "                                             mlp_size= mlp_size,\n",
    "                                             dropout= mlp_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.MSA_Block(x) + x\n",
    "        x = self.MLP_Block(x) + x\n",
    "        x = self.MSA_Block(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34898258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size: int,\n",
    "                 in_channels: int,\n",
    "                 patch_size: int,\n",
    "                 num_transformer_layers: int,\n",
    "                 embedding_dim: int,\n",
    "                 mlp_size: int,\n",
    "                 num_heads: int,\n",
    "                 attn_dropout: float,\n",
    "                 mlp_dropout: float,\n",
    "                 embedding_dropout: float,\n",
    "                 num_classes: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        assert image_size % patch_size == 0, \"patch size is divisible by image size\"\n",
    "\n",
    "        self.num_patches = int(image_size ** 2 / patch_size ** 2)\n",
    "        \n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad= True)\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim),\n",
    "                                               requires_grad= True)\n",
    "        \n",
    "        self.patch_embedding = PatchEmbedding(in_channels= in_channels,\n",
    "                                              patch_size= patch_size,\n",
    "                                              embedding_dim= embedding_dim)\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout(p = embedding_dropout)\n",
    "\n",
    "        self.transformerencoder = nn.Sequential(* [TransformerEncoder(embedding_dim= embedding_dim,\n",
    "                                                     num_heads= num_heads,\n",
    "                                                     mlp_size= mlp_size,\n",
    "                                                     attn_dropout= attn_dropout,\n",
    "                                                     mlp_dropout= mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim = 1)\n",
    "\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformerencoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6c9e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMechBlock(nn.Module):\n",
    "    def __init__(self, dim, units=128):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(dim, units)\n",
    "        self.key = nn.Linear(dim, units)\n",
    "        self.value = nn.Linear(dim, units)\n",
    "        self.LayerNorm = nn.LayerNorm(normalized_shape= units)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn = torch.softmax(Q @ K.transpose(1,2) / (x.size(-1)**0.5), dim=-1)\n",
    "        return self.LayerNorm((attn @ V).mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e14ee2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_size: int,\n",
    "                 in_channels: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int,\n",
    "                 patch_size: int,\n",
    "                 num_transformer_layers: int,\n",
    "                 embedding_dim: int,\n",
    "                 mlp_size: int,\n",
    "                 num_heads: int,\n",
    "                 attn_dropout: float,\n",
    "                 mlp_dropout: float,\n",
    "                 embedding_dropout: float,\n",
    "                 units: int = 128,\n",
    "                 num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.CNNBlock = CNNBlock(input_shape= 3,\n",
    "                                 hidden_units= hidden_units,\n",
    "                                 output_shape= output_shape)\n",
    "        self.ViTBlock = ViTBlock(image_size= image_size,\n",
    "                                 in_channels= in_channels,\n",
    "                                 patch_size= patch_size,\n",
    "                                 num_transformer_layers= num_transformer_layers,\n",
    "                                 embedding_dim= embedding_dim,\n",
    "                                 mlp_size= mlp_size,\n",
    "                                 num_heads= num_heads,\n",
    "                                 attn_dropout= attn_dropout,\n",
    "                                 mlp_dropout= mlp_dropout,\n",
    "                                 embedding_dropout= embedding_dropout,\n",
    "                                 num_classes= num_classes)\n",
    "        self.AttentionMechBlock = AttentionMechBlock(dim= embedding_dim,\n",
    "                                                     units= units)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p = mlp_dropout),\n",
    "            nn.Linear(in_features= units,\n",
    "                      out_features= num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.CNNBlock(x)\n",
    "        x = self.ViTBlock(x)\n",
    "        x = self.AttentionMechBlock(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49653e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = HybridModel(image_size= 10,\n",
    "                    in_channels= 64,\n",
    "                    hidden_units= 32,\n",
    "                    output_shape= 64,\n",
    "                    patch_size= 2,\n",
    "                    num_transformer_layers= 5,\n",
    "                    embedding_dim= 256,\n",
    "                    mlp_size= 1024,\n",
    "                    num_heads= 128,\n",
    "                    attn_dropout= 0,\n",
    "                    mlp_dropout= 0.1,\n",
    "                    embedding_dropout= 0.1,\n",
    "                    units= 128,\n",
    "                    num_classes= 2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b089a80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "HybridModel                                             [32, 2]                   --\n",
       "├─CNNBlock: 1-1                                         [32, 64, 10, 10]          --\n",
       "│    └─Sequential: 2-1                                  [32, 64, 10, 10]          --\n",
       "│    │    └─Conv2d: 3-1                                 [32, 32, 30, 30]          896\n",
       "│    │    └─BatchNorm2d: 3-2                            [32, 32, 30, 30]          64\n",
       "│    │    └─ReLU: 3-3                                   [32, 32, 30, 30]          --\n",
       "│    │    └─Conv2d: 3-4                                 [32, 32, 28, 28]          9,248\n",
       "│    │    └─BatchNorm2d: 3-5                            [32, 32, 28, 28]          64\n",
       "│    │    └─ReLU: 3-6                                   [32, 32, 28, 28]          --\n",
       "│    │    └─MaxPool2d: 3-7                              [32, 32, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-8                                 [32, 64, 10, 10]          51,264\n",
       "│    │    └─BatchNorm2d: 3-9                            [32, 64, 10, 10]          128\n",
       "│    │    └─ReLU: 3-10                                  [32, 64, 10, 10]          --\n",
       "├─ViTBlock: 1-2                                         [32, 26, 256]             6,912\n",
       "│    └─PatchEmbedding: 2-2                              [32, 25, 256]             --\n",
       "│    │    └─Conv2d: 3-11                                [32, 256, 5, 5]           65,792\n",
       "│    │    └─Flatten: 3-12                               [32, 256, 25]             --\n",
       "│    └─Dropout: 2-3                                     [32, 26, 256]             --\n",
       "│    └─Sequential: 2-4                                  [32, 26, 256]             --\n",
       "│    │    └─TransformerEncoder: 3-13                    [32, 26, 256]             789,760\n",
       "│    │    └─TransformerEncoder: 3-14                    [32, 26, 256]             789,760\n",
       "│    │    └─TransformerEncoder: 3-15                    [32, 26, 256]             789,760\n",
       "│    │    └─TransformerEncoder: 3-16                    [32, 26, 256]             789,760\n",
       "│    │    └─TransformerEncoder: 3-17                    [32, 26, 256]             789,760\n",
       "├─AttentionMechBlock: 1-3                               [32, 128]                 --\n",
       "│    └─Linear: 2-5                                      [32, 26, 128]             32,896\n",
       "│    └─Linear: 2-6                                      [32, 26, 128]             32,896\n",
       "│    └─Linear: 2-7                                      [32, 26, 128]             32,896\n",
       "│    └─LayerNorm: 2-8                                   [32, 128]                 256\n",
       "├─Sequential: 1-4                                       [32, 2]                   --\n",
       "│    └─Flatten: 2-9                                     [32, 128]                 --\n",
       "│    └─Dropout: 2-10                                    [32, 128]                 --\n",
       "│    └─Linear: 2-11                                     [32, 2]                   258\n",
       "=========================================================================================================\n",
       "Total params: 4,182,370\n",
       "Trainable params: 4,182,370\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 562.02\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.39\n",
       "Forward/backward pass size (MB): 103.25\n",
       "Params size (MB): 11.44\n",
       "Estimated Total Size (MB): 115.08\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Model,\n",
    "        input_size= (32, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe57aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YUVRAJ\\ML\\Projects_end-to-end\\AI_image_classifier\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer= optimizer,\n",
    "    mode= \"min\",\n",
    "    factor= 0.5,\n",
    "    patience= 5,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts import engine\n",
    "\n",
    "wandb.init(project=\"AI_Image_Classification\", name=\"Hybrid_model_01\", settings=wandb.Settings(symlink=False))\n",
    "wandb.config.epochs = 10\n",
    "wandb.config.batch_size = 32\n",
    "\n",
    "config = wandb.config\n",
    "results = { \n",
    "        \"train loss\": [],\n",
    "        \"train acc\": [],\n",
    "        \"test loss\": [],\n",
    "        \"test acc\": []\n",
    "    }\n",
    "    \n",
    "train_dataloader = DataLoader(train_data,\n",
    "                            batch_size= config.batch_size,\n",
    "                            shuffle= True)\n",
    "val_dataloader = DataLoader(val_data,\n",
    "                            batch_size= config.batch_size)\n",
    "test_dataloader = DataLoader(test_data,\n",
    "                             batch_size= config.batch_size)\n",
    "\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    train_loss, train_acc,y_train_actual, y_train_predicted = engine.train_loop(model= Model,\n",
    "                                                                                train_dataloader= train_dataloader,\n",
    "                                                                                loss_fn= loss_func,\n",
    "                                                                                optimizer= optimizer,\n",
    "                                                                                device= device)\n",
    "    test_loss, test_acc, y_test_actual, y_test_predicted = engine.test_loop(model= Model,\n",
    "                                                                            test_dataloader= val_dataloader,\n",
    "                                                                            loss_fn= loss_func,\n",
    "                                                                            device= device)\n",
    "    \n",
    "    results[\"train loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "    results[\"train acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "    results[\"test loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "    results[\"test acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\" : epoch + 1,\n",
    "        \"train_loss\" : train_loss,\n",
    "        \"train_accuracy\" : train_acc,\n",
    "        \"test_loss\" : test_loss,\n",
    "        \"test_accuracy\" : test_acc,\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{config.epochs}: train loss: {train_loss:.4f} |\\ntrain accuracy: {train_acc:.4f} |\\ntest loss: {test_loss:.4f} |\\ntest accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    torch.onnx.export(\n",
    "        Model,\n",
    "        torch.randn(1,3,224,224),\n",
    "        \"Models\\model.onnx\",\n",
    "        input_names = [\"input\"],\n",
    "        output_names = [\"output\"],\n",
    "    )\n",
    "\n",
    "wandb.log_artifact(\"Models\\hybridmodel.onnx\", name= \"Hybrid_model_01\", type= \"model\")\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900739da",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_train_actual, y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test_actual, y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b185cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train_actual, y_train_predicted)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_actual, y_test_predicted)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
